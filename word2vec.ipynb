{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2953a83d",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537711a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafc395",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e07295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Lowercase and remove punctuation\n",
    "to_remove = ['.', ',', '?', '!', ':', ';']\n",
    "replacement = \" \"\n",
    "\n",
    "pattern = \"|\".join(map(re.escape, to_remove))\n",
    "df[\"text\"] = df[\"text\"].str.replace(pattern, replacement, regex=True)\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ad8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Samples with too Many Words\n",
    "print(df.shape)\n",
    "max_words = 100\n",
    "df = df[df['text'].str.split().apply(len) <= max_words]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6ced5",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "data = []\n",
    "\n",
    "# Tokenize the text data\n",
    "for i in range(len(df)):\n",
    "    temp = []\n",
    "    for j in range(len(df['text'].iloc[i].split())):\n",
    "        temp.append(df['text'].iloc[i].split()[j])\n",
    "    data.append(temp)\n",
    "\n",
    "print(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06970b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "wvmodel = gensim.models.Word2Vec(data, min_count=1,vector_size=100, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c176dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'chicken'\n",
    "word2 = 'butter'\n",
    "\n",
    "word_sim = wvmodel.wv.similarity(word1, word2) * 100\n",
    "word_sim = word_sim\n",
    "\n",
    "print(f\"Similarity between {word1} and {word2}: {word_sim : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388cacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wvmodel.wv.most_similar('penetrate', topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fe422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# Load model\n",
    "word2vec_model = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tokens_list = []\n",
    "vectorized_samples = []\n",
    "kept_samples = []\n",
    "\n",
    "for i, quote in enumerate(df['text']):\n",
    "    tokens = simple_preprocess(quote)\n",
    "    tokens_list.append(tokens)\n",
    "    word_vectors = [word2vec_model[token] for token in tokens if token in word2vec_model]\n",
    "    \n",
    "    # Skip if no valid word vectors , empty or too long\n",
    "    if len(word_vectors) == 0 or len(word_vectors) > 5000:\n",
    "        kept_samples.append(False)\n",
    "        continue\n",
    "    \n",
    "    kept_samples.append(True)\n",
    "    vectorized_samples.append(torch.tensor(word_vectors))\n",
    "\n",
    "padded_sequence = pad_sequence(vectorized_samples, batch_first=True)\n",
    "print(f\"Padded sequence shape: {padded_sequence.shape}\")\n",
    "print(f\"Kept {len(vectorized_samples)} out of {len(df)} samples\")\n",
    "print(f\"First sample shape: {padded_sequence[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from PositionalEncoding import PositionalEncoding\n",
    "\n",
    "class TransformerModel(nn.Transformer):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, noutput, dropout=0.3):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.ninp = ninp\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.decoder = nn.Linear(ninp * ntoken, noutput)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.flatten(output)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "X = padded_sequence.to(device)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['author'][kept_samples].values.reshape(-1, 1))\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "dataset = TensorDataset(X, y)\n",
    "model = TransformerModel(X.shape[1], ninp=100, nhead=5, nhid=512, nlayers=3, noutput=3, dropout=0.3).to(device)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function to minimize validation loss\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    nhid = trial.suggest_int('nhid', 128, 768, step=128)  # 128, 256, 384, 512, 640, 768\n",
    "    nlayers = trial.suggest_int('nlayers', 1, 4)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
    "    \n",
    "    # Create datasets with suggested batch size\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create model with suggested hyperparameters\n",
    "    model = TransformerModel(\n",
    "        X.shape[1], \n",
    "        ninp=100,  # Keep embedding size fixed\n",
    "        nhead=5,   # Keep nhead fixed (must divide ninp)\n",
    "        nhid=nhid, \n",
    "        nlayers=nlayers, \n",
    "        noutput=3, \n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_dataloader:\n",
    "                outputs = model.forward(X_batch, has_mask=False)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # Report intermediate value for pruning\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Create study and optimize\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study.optimize(objective, n_trials=50, timeout=7200)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best validation loss: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "fig = plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "# Visualize parameter importance\n",
    "fig = plot_param_importances(study)\n",
    "fig.show()\n",
    "\n",
    "# Show all trials\n",
    "trials_df = study.trials_dataframe()\n",
    "print(\"\\nAll trials:\")\n",
    "print(trials_df[['number', 'value', 'params_nhid', 'params_nlayers', 'params_dropout', 'params_lr', 'params_batch_size']].sort_values('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Create final model\n",
    "final_model = TransformerModel(\n",
    "    X.shape[1], \n",
    "    ninp=100, \n",
    "    nhead=5, \n",
    "    nhid=best_params['nhid'], \n",
    "    nlayers=best_params['nlayers'], \n",
    "    noutput=3, \n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Create dataloaders with best batch size\n",
    "final_train_dataloader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "final_val_dataloader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    final_model.parameters(), \n",
    "    lr=best_params['lr'], \n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Train for more epochs with best params\n",
    "num_epochs = 50\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Training\n",
    "    final_model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in final_train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model.forward(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(final_train_dataloader)\n",
    "    training_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in final_val_dataloader:\n",
    "            outputs = final_model.forward(X_batch, has_mask=False)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(final_val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(final_model.state_dict(), \"transformer_model_best_optuna.pt\")\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}, LR: {current_lr:.6f}')\n",
    "\n",
    "# Plot final results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Final Model Training (Best Params from Optuna)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%matplotlib inline\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "X = padded_sequence.to(device)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['author'][kept_samples].values.reshape(-1, 1))\n",
    "y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "print(y.shape)\n",
    "print(X.shape[1])\n",
    "dataset = TensorDataset(X, y)\n",
    "model = TransformerModel(X.shape[1], ninp=100, nhead=5, nhid=512, nlayers=3, noutput=3, dropout=0.3).to(device)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "num_epochs = 40\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model.forward(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    training_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_dataloader:\n",
    "            outputs = model.forward(X_batch, has_mask=False)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            validation_loss += loss.item()\n",
    "    validation_loss /= len(val_dataloader)\n",
    "    val_losses.append(validation_loss)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {validation_loss:.4f}')\n",
    "\n",
    "# Plot both training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), \"transformer_model.pt\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32214f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model.load_state_dict(torch.load(\"transformer_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Model\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_dataloader:\n",
    "        outputs = model.forward(X_batch, has_mask=False)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total : .2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_installed",
   "language": "python",
   "name": "kernel_installed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
